{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "version='v0.1'\n",
    "app_tr_path=\"..\\\\data\\\\home-credit-default-risk\\\\application_train.csv\"\n",
    "app_te_path=\"..\\\\data\\\\home-credit-default-risk\\\\application_test.csv\"\n",
    "\n",
    "app_train = pd.read_csv(app_tr_path)\n",
    "app_test = pd.read_csv(app_te_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Missing Values</th>\n",
       "      <th>% of Total Values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>COMMONAREA_MEDI</th>\n",
       "      <td>214865</td>\n",
       "      <td>69.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMMONAREA_AVG</th>\n",
       "      <td>214865</td>\n",
       "      <td>69.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>COMMONAREA_MODE</th>\n",
       "      <td>214865</td>\n",
       "      <td>69.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONLIVINGAPARTMENTS_MODE</th>\n",
       "      <td>213514</td>\n",
       "      <td>69.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NONLIVINGAPARTMENTS_AVG</th>\n",
       "      <td>213514</td>\n",
       "      <td>69.4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Missing Values  % of Total Values\n",
       "COMMONAREA_MEDI                   214865               69.9\n",
       "COMMONAREA_AVG                    214865               69.9\n",
       "COMMONAREA_MODE                   214865               69.9\n",
       "NONLIVINGAPARTMENTS_MODE          213514               69.4\n",
       "NONLIVINGAPARTMENTS_AVG           213514               69.4"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def missing_values_table(df):\n",
    "    mis_val = df.isnull().sum()\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns.sort_values(\n",
    "    '% of Total Values', ascending=False).round(1)\n",
    "    return mis_val_table_ren_columns\n",
    "\n",
    "missing_values = missing_values_table(app_train)\n",
    "missing_values.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "le_count = 0\n",
    "for col in app_train:\n",
    "    if app_train[col].dtype == 'object':\n",
    "        if len(list(app_train[col].unique())) <= 2:\n",
    "            le.fit(app_train[col])\n",
    "            app_train[col] = le.transform(app_train[col])\n",
    "            app_test[col] = le.transform(app_test[col])\n",
    "            le_count += 1\n",
    "\n",
    "app_train = pd.get_dummies(app_train)\n",
    "app_test = pd.get_dummies(app_test)\n",
    "\n",
    "train_labels = app_train['TARGET']\n",
    "app_train, app_test = app_train.align(app_test, join = 'inner', axis = 1)\n",
    "app_train['TARGET'] = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_numeric(df, parent_var, df_name):\n",
    "    \n",
    "    for col in df:\n",
    "        if col != parent_var and 'SK_ID' in col:\n",
    "            df = df.drop(columns = col)\n",
    "            \n",
    "    parent_ids = df[parent_var].copy()\n",
    "    numeric_df = df.select_dtypes('number').copy()\n",
    "    numeric_df[parent_var] = parent_ids\n",
    "    agg = numeric_df.groupby(parent_var).agg(['count', 'mean', 'max', 'min', 'sum'])\n",
    "\n",
    "    columns = []\n",
    "    for var in agg.columns.levels[0]:\n",
    "        if var != parent_var:\n",
    "            for stat in agg.columns.levels[1]:\n",
    "                columns.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    agg.columns = columns\n",
    "    \n",
    "    _, idx = np.unique(agg, axis = 1, return_index=True)\n",
    "    agg = agg.iloc[:, idx]\n",
    "    \n",
    "    return agg\n",
    "\n",
    "def agg_categorical(df, parent_var, df_name):\n",
    "    \n",
    "    categorical = pd.get_dummies(df.select_dtypes('category'))\n",
    "    categorical[parent_var] = df[parent_var]\n",
    "    categorical = categorical.groupby(parent_var).agg(['sum', 'count', 'mean'])\n",
    "    \n",
    "    column_names = []\n",
    "    for var in categorical.columns.levels[0]:\n",
    "        for stat in ['sum', 'count', 'mean']:\n",
    "            column_names.append('%s_%s_%s' % (df_name, var, stat))\n",
    "    categorical.columns = column_names\n",
    "    \n",
    "    _, idx = np.unique(categorical, axis = 1, return_index = True)\n",
    "    categorical = categorical.iloc[:, idx]\n",
    "    \n",
    "    return categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kde_target(var_name, df):\n",
    "    \n",
    "    corr = df['TARGET'].corr(df[var_name])\n",
    "    avg_repaid = df.loc[df['TARGET'] == 0, var_name].median()\n",
    "    avg_not_repaid = df.loc[df['TARGET'] == 1, var_name].median()\n",
    "    \n",
    "    plt.figure(figsize = (12, 6))\n",
    "    sns.kdeplot(df.loc[df['TARGET'] == 0, var_name], label = 'TARGET == 0')\n",
    "    sns.kdeplot(df.loc[df['TARGET'] == 1, var_name], label = 'TARGET == 1')\n",
    "\n",
    "    plt.xlabel(var_name); plt.ylabel('Density'); plt.title('%s Distribution' % var_name)\n",
    "    plt.legend() \n",
    "    \n",
    "    print('The correlation between %s and the TARGET is %0.4f' % (var_name, corr))\n",
    "    print('Median value for loan that was not repaid = %0.4f' % avg_not_repaid)\n",
    "    print('Median value for loan that was repaid =     %0.4f' % avg_repaid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def return_size(df):\n",
    "    return round(sys.getsizeof(df) / 1e9, 2)\n",
    "\n",
    "def convert_types(df, print_info = False):\n",
    "    \n",
    "    original_memory = df.memory_usage().sum()\n",
    "    \n",
    "    for c in df:\n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
    "            df[c] = df[c].astype('category')\n",
    "        elif list(df[c].unique()) == [1, 0]:\n",
    "            df[c] = df[c].astype(bool)\n",
    "        elif df[c].dtype == float:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "        elif df[c].dtype == int:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    new_memory = df.memory_usage().sum()\n",
    "    \n",
    "    if print_info:\n",
    "        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n",
    "        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Memory Usage: 0.49 gb.\n",
      "New Memory Usage: 0.18 gb.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2919"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous = pd.read_csv('../data/home-credit-default-risk/previous_application.csv')\n",
    "previous = convert_types(previous, print_info=True)\n",
    "previous_agg = agg_numeric(previous, 'SK_ID_CURR', 'previous')\n",
    "previous_counts = agg_categorical(previous, 'SK_ID_CURR', 'previous')\n",
    "\n",
    "train = app_train\n",
    "train = convert_types(train)\n",
    "test = app_test\n",
    "test = convert_types(test)\n",
    "\n",
    "train = train.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\n",
    "train = train.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "test = test.merge(previous_counts, on ='SK_ID_CURR', how = 'left')\n",
    "test = test.merge(previous_agg, on = 'SK_ID_CURR', how = 'left')\n",
    "\n",
    "gc.enable()\n",
    "del previous, previous_agg, previous_counts\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 605)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 599)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_missing_columns(train, test, threshold = 75):\n",
    "\n",
    "    train_miss = train.isnull().sum()\n",
    "    test_miss = test.isnull().sum()\n",
    "\n",
    "    train_index_list = train_miss.index.tolist()\n",
    "    missing_train_columns=[]\n",
    "    for i in range(train_miss.shape[0]) :\n",
    "        if 100 * train_miss[i] / len(train)>75:\n",
    "            missing_train_columns.append(train_index_list[i])\n",
    "\n",
    "    test_index_list = test_miss.index.tolist()\n",
    "    missing_test_columns=[]\n",
    "    for i in range(test_miss.shape[0]) :\n",
    "        if 100 * test_miss[i] / len(test)>75:\n",
    "            missing_test_columns.append(test_index_list[i])\n",
    "    \n",
    "    missing_columns = list(set(missing_train_columns + missing_test_columns))\n",
    "\n",
    "    train = train.drop(columns = missing_columns)\n",
    "    test = test.drop(columns = missing_columns)\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "train, test = remove_missing_columns(train, test)\n",
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.9\n",
    "\n",
    "corr_matrix = train.corr().abs()\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool_))\n",
    "to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "\n",
    "train = train.drop(columns = to_drop)\n",
    "test = test.drop(columns = to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 508)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10:52:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimatores\" } are not used.\n",
      "\n",
      "[[0.9538178  0.04618224]\n",
      " [0.86457473 0.13542528]\n",
      " [0.97555155 0.02444843]\n",
      " ...\n",
      " [0.9697569  0.03024312]\n",
      " [0.9441286  0.05587142]\n",
      " [0.7804461  0.21955387]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import xgboost\n",
    "train_t=train.copy()\n",
    "train_t = train_t.drop(columns = ['TARGET'])\n",
    "x = train_t.values\n",
    "y = train['TARGET'].values\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2 , random_state = 5)\n",
    "model = xgboost.XGBClassifier(n_estimatores = 200, max_depth = 8, subsample = 0.8, colsample_bytree = 0.8, \n",
    "                              min_child_weight = 50, random_state=27).fit(x_train, y_train)\n",
    "#生成train数据放进模型后的答案\n",
    "ans = model.predict_proba(x_test)\n",
    "\n",
    "ans_test = model.predict_proba(test.values)\n",
    "print(ans_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48744, 2)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#生成答案\n",
    "index=test['SK_ID_CURR']\n",
    "ans=pd.DataFrame(ans_test[:,1],columns = ['TARGET'])\n",
    "result=pd.concat([index,ans],axis=1)\n",
    "result.to_csv('../result/'+version+'/submission_t.csv',encoding = 'utf-8',index = 0)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "df = pd.read_csv('../data/app_tr.csv')\n",
    "df_te = pd.read_csv('../data/app_te.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "for col in df:\n",
    "    if df[col].dtype == 'object':\n",
    "        if len(list(df[col].unique())) <= 2:\n",
    "            le.fit(df[col])\n",
    "            df[col] = le.transform(df[col])\n",
    "            df_te[col] = le.transform(df_te[col])\n",
    "\n",
    "df = pd.get_dummies(df)\n",
    "df_te = pd.get_dummies(df_te)\n",
    "\n",
    "train_labels = df['TARGET']\n",
    "df, df_te = df.align(df_te, join = 'inner', axis = 1)\n",
    "df['TARGET'] = train_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "df_te= np.array(df_te)\n",
    "x_test = df_te[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([307511, 237])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.utils.data as data\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data_root, data_label):\n",
    "        super(MyDataset, self).__init__()\n",
    "        self.data = data_root\n",
    "        self.label = data_label\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data = self.data[index]\n",
    "        labels = self.label[index]\n",
    "        return data, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "\n",
    "    def get_dataset(x,y):\n",
    "        dataset = MyDataset(x, y)\n",
    "        datas = DataLoader(dataset, batch_size=256, shuffle=False, sampler=None, \\\n",
    "                        batch_sampler=None, num_workers=0, collate_fn=None, pin_memory=False, \\\n",
    "                        drop_last=False, timeout=0, worker_init_fn=None, multiprocessing_context=None)\n",
    "        return datas\n",
    "\n",
    "dataset = MyDataset.get_dataset(x_train,y_train)\n",
    "train_size = int(len(dataset)*0.7)\n",
    "val_size = len(dataset)-train_size\n",
    "train, val = data.random_split(dataset, [train_size, val_size])\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type       | Params\n",
      "---------------------------------------\n",
      "0 | encoder | Sequential | 38.9 K\n",
      "---------------------------------------\n",
      "38.9 K    Trainable params\n",
      "0         Non-trainable params\n",
      "38.9 K    Total params\n",
      "0.155     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 1202/1202 [00:08<00:00, 141.48it/s, v_num=3]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 1202/1202 [00:08<00:00, 141.36it/s, v_num=3]\n"
     ]
    }
   ],
   "source": [
    "import lightning as L\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(L.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(237, 128), nn.ReLU(),\n",
    "                                     nn.Linear(128, 64), nn.ReLU(),\n",
    "                                     nn.Linear(64,2))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # in lightning, forward defines the prediction/inference actions\n",
    "        y_pre = self.encoder(x)\n",
    "        return out\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training_step defines the train loop. It is independent of forward\n",
    "        x, y = batch\n",
    "        y_pre = F.log_softmax(self.encoder(x), dim=1)\n",
    "        loss = F.nll_loss(y_pre, y)\n",
    "        self.log(\"train_loss\", loss,on_step=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-3)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "autoencoder = MLP()\n",
    "trainer = L.Trainer(max_epochs=20)\n",
    "trainer.fit(autoencoder, dataset)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]]\n"
     ]
    }
   ],
   "source": [
    "model = autoencoder.eval().cuda(device=0)\n",
    "x=torch.from_numpy(x_test[0]).float().unsqueeze(0).cuda(0)\n",
    "res=model(x).data.cpu().numpy()\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=res[0]\n",
    "target=[]\n",
    "for i in res:\n",
    "    if i[0]<0 | i[1]<0 :\n",
    "        \n",
    "    i=i/i.sum()\n",
    "    target.append(i)\n",
    "type(target)\n",
    "target=np.array(target)\n",
    "type(target)\n",
    "#生成答案\n",
    "test = pd.read_csv('../data/app_te.csv')\n",
    "index=test['SK_ID_CURR']\n",
    "ans=pd.DataFrame(target[:,1],columns = ['TARGET'])\n",
    "result=pd.concat([index,ans],axis=1)\n",
    "result.to_csv('../result/'+version+'/submission_n.csv',encoding = 'utf-8',index = 0)\n",
    "result.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
